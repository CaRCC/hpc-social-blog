---
author: Research Computing Teams Newsletter
blog_subtitle: Helping managers of RCD and R&amp;D computing teams be more effective
  and less stressed
blog_title: Research Computing Teams
blog_url: https://www.researchcomputingteams.org
category: rct-newsletter
date: '2022-04-23 00:00:00'
layout: post
original_url: https://www.researchcomputingteams.org/newsletter_issues/0119.html
title: '#119 - 23 Apr 2022'
---

When Research stops in Research Computing; Onboarding, Mentoring, and Listening; Reverse Interview your New Team; SQLite for hash lookups; Memray for Python Memory Monitoring; Scoped GitHub Credentials; Cori makes the case for HPC disaggregation

              <!-- markdownlint-disable MD033 -->
<!-- markdownlint-disable MD049 -->

<p>Hi!</p>

<p>I wanted to talk a little bit more about Research Computing and Data (RCD), how it connects to other work, and how that informs how I think about when RCD work is and isn’t a research output.</p>

<p>First, I want to make a claim that most readers here would likely agree with, but may not yet be universally accepted elsewhere:  RCD is a discipline in and of itself.  It is a body of knowledge and practice, with full time practitioners, conferences, knowledge creation, sharing, and — increasingly — degree programs.</p>

<p>In fact, I’d make the argument that RCD been one of the most productive and influential academic disciplines of the last several decades.  And an academic discipline it is - I don’t think anyone would doubt that the development of the FFT, or finite element methods, or new methods for processing next generation sequencing data or supporting digital humanities work, are real and significant research and intellectual accomplishments.</p>

<p>Research Computing and Data is a bit unusual in that it’s <em>exclusively an applied discipline</em>.  There’s no real “pure” or “basic” RCD research work.  Such work would fall into the fields of computer science (which is arguably a discipline that is itself a child of RCD) or applied mathematics or maybe electrical engineering (which are parents).</p>

<p>Instead, RCD work is always by its nature applied to some other area.  And while being an applied-only field is a bit unusual, RCD isn’t completely alone in this.  Engineering or even Statistics are overwhelmingly applied fields, but few would dismiss either of them as not being real fields of human endeavour, with real research outputs.</p>

<p>So let’s take it as given that RCD is a real discipline.  I’d claim that the nature of disciplines is that they have an R&amp;D practice.  They have a spectrum of activities — all valuable! — which range from research on one end, to product development at the other.  Crucially, there’s a point - maybe ill-defined, but it exists - at which the work stops being research itself and is now product development, just as is true for chemists, or materials scientists, or economists, or those in other fields.  In those cases, the work stops being the <strong>creation</strong> of new knowledge in their fields, knowledge consumed principally by fellow experts in that field, and starts being about <strong>embodying</strong> that knowledge into something others can use.  That embodying work is still work that requires expertise, it’s still work that builds on the history of work in the practitioners field, but it’s development of a product for use by others that needn’t be experts in that field.</p>

<p>Let’s say a materials scientist is doing some work that looks like it might lead to a new kind of sensor, that could be used in new kinds of lab equipment.  Initial discovery work showing that it could work as a proof of concept, or even as a prototype, probably involves a lot of research work.  The primary output is new knowledge about materials science, knowledge published in journals and that other materials scientists can build on.</p>

<p>Let’s then say the team then contributes to work to commercialize that knowledge, to actually build the sensors that can be incorporated into new lab equipment.  At that point the work stops being principally about producing new materials science knowledge and starts being about creating a new product that relies on, that contains, that embodies, that knowledge.  There will be lots of careful, methodical, expert-level work done building packaging that doesn’t compromise the properties that make the material a good sensor, of building sensitive and reliable ways of measuring output from the packaged sensor.  That’s excellent and important work, and will be used in lab equipment for research, but relies much more heavily now on existing best practices built up from previous product-building work.  It need not and likely will not lead to the development of new materials science knowledge.  It doesn’t generate materials science research outputs.  That’s true even if it will be used to help support research in other fields.</p>

<p>This research-to-development journey doesn’t have to be strictly linear!  It could be that in building the packaging new materials science knowledge is required.  And to develop the next version of the sensor, it’s certainly possible that new materials science knowledge ends up being developed, generating materials science research outputs.  But it’s also possible that it doesn’t, and that the work done is purely product work, developing a product rather than new materials science knowledge.</p>

<p>I think about research computing and data work, the work of research computing teams, the same way.  Because RCD is always necessarily applied, I sometimes find it useful to think of it not as a one-dimensional research to development spectrum, but in a two-dimensional space - considering how mature the RCD work itself is, and how mature the problem is it’s being applied to are.</p>

<p><img alt="Research Computing and Data work ranging from research to product development: a two dimensional diagram, described below, considering maturity of the problem being considered on the top, ranging from novel on the left to mature on the right, and maturity of the RCD proposed solution on the side, ranging from novel at the top to mature at the bottom." src="https://buttondown.s3.amazonaws.com/images/505069a3-827a-4947-b963-0c4a524a653f.png" /></p>

<p>In the case of creating a new sensor, the problem is evolving and becoming clearer along with the product itself - the path lies more or less along the diagonal.  That’s also how I feel about much of bioinformatics, a field that has grown in parallel with — both being driven by and in turn driving — developments in high-throughput molecular biology data collection.   Along that diagnal there is a very exploratory phase, where you’re not even sure what questions make sense to ask, through to turning things into prototypes, through to delivering a mature product.   Along that diagonal (which again needn’t be followed strictly linearly) work changes from producing mostly new RCD knowledge to packaging that knowledge into a product that non-RCD experts can use.</p>

<p>But there’s alternatives to developing tools that grow in maturity with the field.  In the top-right corner, with new RCD solutions but mature problems, new methods can be applied to extremely mature fields — a new numerical method for fluid dynamics, for instance.  Novel methods, in any field, are more or less always recognized as research outputs.  In these cases “product development” is much simpler - going from that proof of concept to something in production is likely something that can happen quite quickly, as the needs of users are very well understood, and there’s probably existing tooling you can build on to deliver the new method into the hands of users really quickly.   On the other hand you could build new frameworks for existing methods; that’s useful, but is less likely to be a research output, building more on software development best practices than generating new RCD knowledge.</p>

<p>In other corner, you have mature RCD outputs being applied to new problems.  This is the work of translation; of realizing an existing method can be applied to answer new questions.  That work can quite possibly generate both a new product and new RCD knowledge.</p>

<p>I won’t claim that this is The Answer to RCD as a research output, but it’s a robust, self-consistent and well-motivated take.  I think we haven’t thought about RCD in terms of R&amp;D mainly because up to this point, we’ve mainly been thinking of RCD solely in terms of research software development.  Since we want research software by and large to be open source, a commercialization approach seems weird; and as long as grant funding rates were really good, it was easier just to treat RCD products as research outputs and get them funded and recognized following that approach.</p>

<p>But this is growing untenable, for a number of reasons:</p>

<ul>
  <li>As research software products get more complex and more teams depend on them, the project-based approach that serves the funding of <em>e.g.</em> programmes of experiments becomes more and more unsuitable,</li>
  <li>Funding success rates are plummeting across the board anyway, and especially for any work that can be dismissed as “incremental”, such as maintenance of existing tools (this devaluing of incremental work causes problems well beyond its impacts on RCD, but that’s a whole different article),</li>
  <li>Research data curation and maintaining research data resources is increasingly important, and that works fits the research project mould more poorly even than research software development did,</li>
  <li>Research software, systems, and data are becoming more closely intertwined, it makes less sense than ever to consider them separately, and research systems are clearly mostly products.</li>
</ul>

<p>I don’t know what the solution to any of the above is.  But thinking more clearly about the purpose of our work and what success looks like, and RCD’s place as a discipline, is more likely to get us there than just continuing to hope that RCD work can be funded in the same way as doing experiments or theoretical analyses.</p>

<p>Sorry, that was long!  Now on to the roundup.</p>

<h2 id="managing-teams">Managing Teams</h2>

<p><a href="https://hbr.org/2022/04/onboarding-can-make-or-break-a-new-hires-experience">Onboarding Can Make or Break a New Hire’s Experience</a> - Sinazo Sibisi and Gys Kappers</p>

<p>I’m obviously thinking a lot about onboarding lately!</p>

<p>When we do hire, we often put a lot of work and effort into actually finding and hiring someone, and then the work kind of … stops.  There’s not a lot of effort put into the “on ramp” to the job, getting people geared up and ready to succeed in their first three months or so.  That makes the new hires unhappy - you’ve presumably made a point of hiring someone who is a high-achiever and wants to get stuff done - and it makes the other team members unhappy as it takes a long time for the new hire to come up to speed and pull their weight.</p>

<p>This situation is particularly dire in research computing, where we don’t get to hire very often and so any onboarding plan and materials we might have are probably way out of date.  One of the reasons I really like hiring student interns year around is that it presents a huge opportunity (if taken, which I didn’t always do!) to get the hiring/onboarding/offboarding processes running really smoothly, and to improve it over time.</p>

<p>Sibisi and Kappers walk through their recommendations for improving onboarding.  Like so much in managing, it’s not rocket science and the recommendations aren’t huge flashes of insight - it’s paying attention to the details and doing the work</p>

<ul>
  <li>Setting goals for the onboarding process</li>
  <li>Collaborate across departments, units, (and I’d add stakeholders) the team member would be interacting with</li>
  <li>Make sure the new hire receives support consistently through the process</li>
</ul>

<hr />

<p><a href="https://letsgrowleaders.com/2022/04/18/mentoring-conversations-how-to-be-remarkably-helpful-with-limited-time/?vgo_ee=HJLIy7pxGfNZ2Jv+QvZUhGGj3YyD5H6+PzCZyeXE/dA%3D">Mentoring Conversations: How to Be Remarkably Helpful with Limited Time</a> - Karin Hurt, Let’s Grow Leaders<br />
<a href="https://lauriebrown.com/blog/how-to-improve-your-listening-skills-here-are-five-tips/">How to improve your listening skills? Here are five tips.</a> - Laurie Brown</p>

<p>Honestly I’m pretty down on mentoring in this newsletter, not because it’s bad but because advice is often so cheaply given compared to (say) actively sponsoring a team member for new opportunities.</p>

<p>But.</p>

<p>If you’re deeply thinking about what your mentee wants and ask a lot of questions, particular questioning their assumptions and using something of a more coaching approach to help them find their own answers, it can be really valuable.  Hurt here describes a mentoring conversation that changed her career path.</p>

<p>Relatedly, Brown reminds us that listening well takes a lot of effort and practice; her five steps for listening (in mentoring or other conversations) are:</p>

<ul>
  <li>Focus</li>
  <li>Pay attention to their body language, emotional tenor, and to what’s not being said</li>
  <li>Acknowledge what they said - paraphrasing back is a very powerful tool</li>
  <li>Inquire to get deeper</li>
  <li>And <em>then</em> respond.</li>
</ul>

<hr />

<h2 id="managing-your-own-career">Managing Your Own Career</h2>

<p><a href="https://blog.pragmaticengineer.com/reverse-interviewing/">Reverse Interviewing Your Future Manager and Team</a> - Gergely Orosz</p>

<p>There’s no perfect jobs, but there are bad jobs, and there are overall good jobs which nonetheless are a very bad match to what you personally prefer and want to be doing every day.</p>

<p>Once you get an offer you can start reverse-interviewing in earnest.  I don’t mean the “what does success look like in this role” kind of question you ask in the last 5 minutes of a 60 minute interview slot, I mean really digging deep to figure out what working in the job would actually be like.  Especially if you have a couple of offers (and todays environment is <strong>very conducive</strong> to getting multiple offers), the information uncovered here can help guide you to a choice you’ll be happy with for a few years.</p>

<p>Besides any specific questions that you have after thinking about the role and the organization - <em>anything</em> which seems odd or surprising to you - Orosz and some of his colleagues contribute some pretty good general purpose questions for the hiring manager but also senior peers you’d be working with:</p>

<ul>
  <li>What are you most excited about for the next six months?</li>
  <li>What did your typical workweek look like for the past month?</li>
  <li>How would you describe the team dynamics</li>
  <li>What advice would you give the new hire to be successful?</li>
  <li>Hsa anyone left the team?  What was the reason?</li>
  <li>Can you show a non-trivial code review [LJD - or migration or update or…]?</li>
  <li>What separates a good day from a bad day for you? (and then dig into how often the bad days occur and what causes them)</li>
  <li>What do you like and dislike about the manager?</li>
  <li>Who did they last promote on the team, and why?</li>
  <li>How supportive are they as a manager?</li>
</ul>

<p>Even if the answers only solidify your decision to join the new company, they will be useful for giving you a head start, knowing better what to expect in your first months.</p>

<hr />

<h2 id="product-management-and-working-with-research-communities">Product Management and Working with Research Communities</h2>

<p><a href="https://www.xsede.org/advancetoaccess">National Science Foundation Announces ACCESS Awards</a> - Dina Meek, NCSA</p>

<p>ACCESS, the follow-on to XSEDE, is starting to take shape as $52 million over five years for (as I understand it) personnel and models for service delivery are awarded by NSF.</p>

<p>In NSFs call there were four separate tracks, for handling allocation services, for handling user services, for operations and integration between the different systems, and for monitoring and measurement services.  (I have to admit, as an outsider, it’s not obvious to me why the last two would be separate).  Then there’s an overall coronation office, which will be run out of NCSA.</p>

<p>With the awards just announced, details are sketchy - XSEDE will be active through August, so ACCESS will have several months to ramp up, and presumably we’ll learn more over the summer.   But even with the press-release-level information, I’m fascinated by the most user-facing two of these: <a href="https://www.psc.edu/psc-and-partners-to-lead-nsf-access/">RAMPS (out of PSC)</a> for allocation, and <a href="https://amp.ci">MATCH</a> for user support.</p>

<p>RAMPS looks like there are a number of useful ideas that will come into play:</p>

<ul>
  <li>“Establishing a hierarchy of opportunities so that smaller amounts of needed computational time will be accessible through simpler, quicker applications” - this will be better for researchers just getting started or that just don’t need lots of compute.  It will be more work for the reviewers but better for researchers, which is a tradeoff we too often don’t make; and</li>
  <li>“Structuring the awards system so that, though users’ proposals will still be awarded based on their own merit, the NSF HPC resources will compete for users rather than the other way around.” - I <em>REALLY</em> want to see how this plays out.  Far too often researchers basically have no choice in where they run, which means we get no “revealed preference” information about what they value.  Giving researchers who don’t need highly-specialized resources “Access bucks” or whatever to spend where they want (if that’s what’s being described here, which I honestly don’t know) would be a huge shift in dynamics and would result in systems better tuned to what researchers actually value.</li>
</ul>

<p>Do we have any readers who know more about the details of MATCH or RAMPS (or the other awardees) who are comfortable sharing some information?</p>

<hr />

<p>As in-person conferences recommence, regional conferences of running scientific core facilities (like this one <a href="https://madssci.abrf.org/program/">for the mid-atlantic states</a> in the US) are beginning to start back up.  The managers and staff of these core facilities have a lot of expertise in sustainably providing service offerings to research, and lots of experience in service development, service lifecycles, and business case modelling.  I’m not sure I’d recommend making a trip just to attend one, but if there’s one at or near your institution it could be a useful way to network with other leaders and managers facing similar challenges as RCD teams, in a different context.</p>

<hr />

<h2 id="research-software-development">Research Software Development</h2>

<p><a href="http://ivory.idyll.org/blog/2022-storing-ulong-in-sqlite-sourmash.html">Storing 64-bit unsigned integers in SQLite databases, for fun and profit</a> - C. Titus Brown</p>

<p>A blog post about research software development that touches on the challenges of working on a giant pull request, using SQLite in a research software code, <em>and</em> discussion of tradeoffs in different approaches is, as longtime friend of the newsletter Brown must know by now, too much for me to resist including here.</p>

<p>In this article, Brown describes moving <a href="https://sourmash.readthedocs.io/en/latest/">sourmash</a> away from the custom-built inverted index and sequence bloom tree into using SQLite as a searchable backing store connecting kmer hashes to the sequences containing the kmers.  These get quite large (4.6 billion hashes), so efficient approahes are important; but efficiency means a few things here, ideally it would be low disk usage and fast and low memory usage, and of the three likely at least one will have to give.  Hashes are an interesting problem for this, too, because since they’re hashes they don’t support (say) range searches; locality doesn’t mean anything in the hashed space.</p>

<p>Longtime - or heck even recent! - readers will know that I think embedded databases are criminally underused in research software, and Brown too really likes SQLite in particular, so he wanted to see if SQLite could be used here.  That’s not trivial because SQLite, while great, has actually pretty weak support for types, and in particular doesn’t support 64-bit unsigned ints at all.  The first appraoch didn’t go well!  But by understanding the structure of the data - in partiuclar since order didn’t matter with the hashes, he could type pun unsigned ints into signed ints as long as it was done consistently - he got very good results, consistent with the tradeoffs that made sense for the sourmash product (allowing more diskspace use for the index if it makes lookups faster while keeping memory useage down).  There’s also some solid PRAGMA choies described for read-only workloads like this one.</p>

<p>It’s a good article to read if you’re thinking of taking advantage of sqlite or other embedded databases for data lookups as opposed to “rolling your own”.</p>

<hr />

<p><a href="https://writing.kemitchell.com/2022/04/22/EULAs-Arent-Inherently-Evil">EULAs Aren’t Inherently Evil</a> - Kyle E. Mitchell</p>

<p>Mitchell is a tech lawyer who the newsletter has linked to in discussions of open source development in the past (such as #<a href="https://www.researchcomputingteams.org/newsletter_issues/0083">83</a> on contributor license agreements).  Here he makes the case that non-open-source software isn’t (<em>pace,</em> say, the FSF) inherently evil.</p>

<p>In research computing there’s an overwhelming case for needing source to be available for simulation and data analysis methods, but not all research computing software falls into those categories, and such availability needn’t only be through the traditional open source licenses.</p>

<p>Until we’ve figured out a way to make the development of RCD software products sustainable through traditional funding sources in the long term, we can’t completely rule out supplementary funding models for product development like hosted software-as-a-service, selling support, or just flat out charging for software.  I’m watching infrastructure projects like <a href="https://www.nextflow.io">nextflow</a> with interest, with a sort of “freemium” model of an open source community edition core and then <a href="https://cloud.tower.nf">Sequera Labs</a> charging for hosting and support (and clinical labs actually prefer to pay for support for key tools).   To make that work, we may  have to consider a broader range of licensing models.  I don’t love the idea either.</p>

<hr />

<p>Bloomberg has <a href="https://github.com/bloomberg/memray">open-sourced</a> <a href="https://bloomberg.github.io/memray/index.html">Memray</a>, really useful looking python memory profiling tool that runs standaloneor can be queried via APIs.</p>

<p><img alt="Live-view of memory use in a Python program by line of code in a “top”-like format with memray" src="https://bloomberg.github.io/memray/_images/live_disconnected.png" /></p>

<hr />

<h2 id="research-data-management-and-analysis">Research Data Management and Analysis</h2>

<p><a href="https://blog.jupyter.org/securely-pushing-to-github-from-a-jupyterhub-3ee42dfdc54f">Securely pushing to GitHub from a JupyterHub with gh-scoped-creds</a> - Yuvi Panda</p>

<p>Potentially useful well beyond Jupyter, <a href="http://gh-schoped-creds"><code class="language-plaintext highlighter-rouge">gh-scoped-creds</code></a> is a github app which issues time-limited, repository-scoped credentials for your GitHub account that can be used to push changes in JupyterHub or on other shared systems where you might want to automate some github actions but don’t want your credentials lying around.</p>

<hr />

<p>Push ML methods as far into the database as possible with the open-source <a href="https://github.com/mindsdb/mindsdb">MindsDB</a> - I think this is an under-appreciated approach, and for some use cases will be a significant win over trying to do something clever and resource intensive with distributed systems.</p>

<hr />

<p>An emerging light-weight stripped down alternative to elasticsearch - <a href="https://github.com/valeriansaliou/sonic">sonic</a>.  Cool looking, but as ever, mind how you spend those innovation tokens.</p>

<hr />

<h2 id="research-computing-systems">Research Computing Systems</h2>

<p><a href="https://neilmadden.blog/2022/04/19/psychic-signatures-in-java/">CVE-2022-21449: Psychic Signatures in Java</a> - Neil Madden, ForgeRock</p>

<p>More Java security issues, where new (Java 15+) updated and improved(!!) code for handling ECDSA signatures.  Except it meant that such signatures could be trivially bypassed by the sender, with any signature being “valid” if one parameter is set to zero.   That isn’t great if it’s being used for <em>e.g.</em> JWT validation.  The bug’s been there since the start of 2020, and known since at least Nov 2021, and fixes are just being rolled out now.</p>

<p>Luckily I doubt most of our systems are running such up to date version of Java -  <em>i.e.</em> Keycloak relies on Java 8 or 11.</p>

<hr />

<p>New <a href="https://discourse.ubuntu.com/t/jammy-jellyfish-release-notes/24668">Ubuntu LTS</a> just dropped.  Some notable updates from my point of view is continuing mainstreaming of deep Arm support, OpenSSL moving to 3.0 which disables a lot of legacy algorithms (yay!), a move to nftables, and a major OpenLDAP update.</p>

<hr />

<p>In the new job, I’ve been thinking about slurm installations a lot more than before, when I took it as a given.  So these are new-to-me resources that might not be new to you - <a href="https://gitlab.com/SchedMD/training/docker-scale-out">a docker-compose slurm “cluster”</a> SchedMD uses for testing and training and <a href="https://github.com/ubccr/hpc-toolset-tutorial">one University of Buffalo uses</a>  that also have <em>e.g.</em> OpenOnDemand, OpenXDMod, and ColdFront.</p>

<hr />

<h2 id="emerging-technologies-and-practices">Emerging Technologies and Practices</h2>

<p><a href="https://dl.acm.org/doi/10.1145/3514245">A Case For Intra-rack Resource Disaggregation in HPC</a> - George Michelogiannakis <em>et al</em>, ACM Transactions on Architecture and Code Optimization</p>

<p>Disaggregation of systems has been coming up more and more often in the roundup (just last week in #<a href="https://www.researchcomputingteams.org/newsletter_issues/0118">118</a> as a motivation for Aquila, when talking about two testbed systems in Texas a couple weeks earlier in #<a href="https://www.researchcomputingteams.org/newsletter_issues/0116">116</a>, and as a topic of more than one conference listed in January’s #<a href="https://www.researchcomputingteams.org/newsletter_issues/0105">105</a>).</p>

<p>This paper by Michelogiannakis <em>et al</em> quantifies the potential usefulness on the HPC side, taking a look at real-world data from NSERC’s top-20 cpu and knights-landing <a href="https://docs.nersc.gov/systems/cori/">Cori</a> system, augmented with profiling data from elsewhere on GPU-heavy ML jobs.  (The authors go into quite a bit of detail in the paper - it’s worth reading even if you’re just interested in workload characterization of Cori. )</p>

<p>Anyone who’s been involved in helping researchers take advantage of large computing systems will be familiar with the basic motivating idea here.  While you can often (sometimes with a lot of work!) get a simulation or data analysis code to take full advantage of a node’s cores, <em>or</em> its memory, <em>or</em> its memory bandwidth, <em>or</em> its network bandwidth, <em>or</em> its accelerators, <em>or</em> its local storage, pegging the needle on two or more of those resources at once is harder.  Taking full advantage of all of them at once basically never happens — and if it did, it would mean the code would almost certainly have to be adjusted to be run effectively on another system that had a different mix of those resources.   (This, by the way, is one why a single “utilization” number for a cluster with a mixed workload is a useless metric even if you’re inclined to believe that utilization of a scientific instrument is the sort of thing which should be a metric).</p>

<p>Since all of our systems are running a mix of jobs with a different profile for taking advantage of those resources, there’s no real “one-size-fits-all” best node size.  What if you could “mix and match” within a rack (say), taking some unused cores or memory or GPU from one node and distributing them virtually to another node where they could be used?</p>

<p>In this paper, the authors look at how Cori is actually used, and find that if there were perfectly efficient ways of distributing the disaggregated resources at the rack level, they could cut back on memory or network bandwidth by 5 to 69% per rack and still satisfy worst-case average needs within a rack for their jobs.  That would would free up resources to be used elsewhere - maybe in more racks, maybe by hiring more staff - and also can be used to give rough and ready potential benefits to be weighed against performance and price costs of emerging disaggregation technologies.</p>

<p><img alt="CDF of node-wide memory occupied by jobs within and across nodes - there are significant fractions of jobs using very little memory and of jobs maxing memory out - definitely not “one size fits all”" src="https://dl.acm.org/cms/asset/754b60a1-77c0-4140-b56f-7aa07e27b624/taco1902-29-f02.jpg" /></p>

<hr />

<p><a href="https://arxiv.org/abs/2204.05959">“Smarter” NICs for faster molecular dynamics: a case study</a> - S. Karamati <em>et al</em>, arXiv:2204.05959</p>

<p>(I’ll note before starting that paper describes work using NVIDIA SmartNICs/DPUs.  The kinds of things the authors here are trying to do to take advantage of smarter programmable network cards apply more generally to this class of device, though, and the results here are sort of mixed and took a lot of work - it’s far from an advert for these cards!  So I’m pretty comfortable including this article in the roundup.)</p>

<p>Here Karamati <em>et al</em> try various approaches to improve the performance of a simple application using SmartNICs.  The application they work with is <a href="https://github.com/Mantevo/miniMD">MiniMD</a>, a simple sample application for molecular dynamics demonstrating patterns broadly similar to the much more complex and powerful LAMMPS code.  MiniMD’s main purpose is to be a testbed for playing with various parallelization and programming approaches.</p>

<p>MiniMD is an interesting choice from among the <a href="https://github.com/Mantevo">Mantevo mini-codes</a> because it <em>isn’t</em> principally communications bound (from the github repo: “Any reasonable parallel computer should be able to achieve excellent scaled speedup (weak scaling)”).  It also has pretty a pretty serial per-node compute-and-then-communicate structure which makes overlapping communication and computation tricky.  Finally, there’s not a lot of collective operations (except for an all gather) in MiniMD. So it’s not obvious that putting more smarts in the network card is a route to improving performance of this code.</p>

<p>Sure enough, they don’t get any speedup from the code as it is.  On the other hand, by restructuring the code to separate out ‘maintenance’ work (updating neighbour lists) from the main computation loops, thus increasing the task parallelism and giving the Arm cores in the NIC something useful to do in parallel as a coprocessor, they improve performance by up to 20% (while increasing power consumption only 6-13% because of the lower power consumption of the Arm cores now enlisted in the work).</p>

<p>The authors make the comparison to other kinds of accelerators - taking advantage of them may require code rearchitecture.  I’d add that once that is done, other possibilities arise - for instance, I’d be curious to see how exploiting this newly-found task parallelism might work even on purely CPU systems.</p>

<p><img alt="Figure 10 from Karamati et al, showing their final approach for accellerating MiniMD using their SmartNIC - offloading not just the MPI communication but neighbour list maintenance onto the card.  That got them about a 20% performance improvement, but at the cost of a significant amount of code changes." src="https://buttondown.s3.amazonaws.com/images/40863b5e-314c-4b8b-89d3-0c82ce7a5895.png" /></p>

<hr />

<p>This is pretty cool - Docker now has an experimental <a href="https://neilmadden.blog/2022/04/19/psychic-signatures-in-java/"><code class="language-plaintext highlighter-rouge">docker sbom</code></a> which will attempt to produce a software build of materials for a container image.</p>

<hr />

<p>The singularity project <a href="https://twitter.com/Singularity_CE/status/1516474514133196802">reports on twitter</a> that, thanks to work on unprivileged cgroups, Singularity CE 3.10 will let you do things like assign fractions of CPUs to containers.  I know at least one group that’s playing with having jobs, or even login node sessions be encapsulated in a singularity image - this could be useful for that work, or even just as a template for using unprivileged cgroups in other contexts.</p>

<hr />

<h2 id="random">Random</h2>

<p>The fascinating history of <a href="https://instadeq.com/blog/posts/no-code-history-lotus-improv-spreadsheets-done-right-1991/">Lotus Improv</a>, 1991s most innovative take on spreadsheets, with an unquestionably better underlying take on data and which equally unquestionably failed completely.  (Check out the <em>incredibly</em> late 80s/early 90s video!)</p>

<p>We’re going to see some extremely cool things done with this - <a href="https://archive.org/details/GeneralIndex">The General Index</a>, a downloadable n-gram index (n ranging from 1-5) of 107,233,728 (so far) academic journal articles.</p>

<p>Todo lists in the editor - must we?  Who cares what those weirdos do with their “org mode”?  Fine, ok, whatever.  Here’s <a href="https://github.com/dewyze/vim-tada">Vim Tada</a>, a todo list for vim.</p>

<p>Learn about <a href="https://benjamin.computer/posts/2022-04-22-ZX-coding.html">assembly on the ZX spectrum</a>.</p>

<p>A very complete list of <a href="https://furbo.org/2014/09/03/the-terminal/">terminal tricks for Mac</a>, aimed at those new to the terminal but including some Mac specific things I hadn’t heard of.</p>

<p><a href="https://blog.jpalardy.com/posts/parsing-and-validate-dates-in-awk/">Parsing and validating dates in Awk</a>.  Look, I’m not going to judge, I’ve done unspeakable things with sed.  Think of it as harm reduction, keeping the kids from getting into perl.</p>

<p>Pre-commit hooks for avoiding accidentally committing secrets to your git repos - <a href="https://github.com/sirwart/ripsecrets">ripsecrets</a>.</p>

<p>A student license for <a href="https://virtuallyfun.com/wordpress/2022/04/18/ready-to-run-openvms-vm-student-kit-from-vsi/">OpenVMS</a> on windows.</p>

<p>A lovely interactive visual tutorial on <a href="https://thenumbat.github.io/Exponential-Rotations/">different approaches to rotations in 3D,</a> and maybe the only quaternion explanation you’ll ever see featuring an adorable cartoon cow.</p>

<p>A promising-looking logging SSH gateway for bastion hosts - <a href="https://github.com/warp-tech/warpgate">warpgate</a>.</p>

<p><a href="https://github.com/rqlite/rqlite">rqlite</a> is a distributed relational database built atop of of sqlite.  It <a href="https://github.com/wildarch/jepsen.rqlite/blob/main/doc/blog.md">stood up to Jepsen-style testing</a> which is no small thing.</p>

<p>I’m not sure that <a href="https://pencil.toast.cafe/wt2om7i8t7">implementing a message queue in the shell</a> is a good idea, but it makes for a good tutorial introduction to the topic.</p>

<p>I wouldn’t say that compiling <a href="https://seri.tools/blog/announcing-rust9x/">rust software for Windows95</a> is a bad idea, exactly, but I’m not sure why you’d want to.</p>

<p>On the other hand I’m pretty sure <a href="https://github.com/anishathalye/git-remote-dropbox">using dropbox as a backing store for a git repository</a> is a bad idea, but I guess if you know the risks?</p>

<p>I’m absolutely certain that <a href="https://blogs.oracle.com/datawarehousing/post/cloud-code-repositories-in-autonomous-database">managing code repos from within your database</a> is a terrible idea.</p>

<p>For the love of god do not <a href="https://www.andreinc.net/2021/01/20/writing-your-own-linear-algebra-matrix-library-in-c">implement your own linear algebra library in C</a>.</p>

<hr />

<h2 id="thats-it">That’s it…</h2>

<p>And that’s it for another week.  Let me know what you thought, or if you have anything you’d like to share about the newsletter or management.  Just <a href="mailto:jonathan@researchcomputingteams.org">email me</a> or reply to this newsletter if you get it in your inbox.</p>

<p>Have a great weekend, and good luck in the coming week with your research computing team,</p>

<p>Jonathan</p>

<h3 id="about-this-newsletter">About This Newsletter</h3>

<p>Research computing - the intertwined streams of software development, systems, data management and analysis - is much more than technology.  It’s teams, it’s communities, it’s product management - it’s people.  It’s also one of the most important ways we can be supporting science, scholarship, and R&amp;D today.</p>

<p>So research computing teams are too important to research to be managed poorly.  But no one teaches us how to be effective managers and leaders in academia.  We have an advantage, though - working in research collaborations have taught us the advanced management skills, but not the basics.</p>

<p>This newsletter focusses on providing new and experienced research computing and data managers the tools they need to be good managers without the stress, and to help their teams achieve great results and grow their careers.</p>