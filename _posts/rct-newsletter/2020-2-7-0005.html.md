---
author: Research Computing Teams Newsletter
blog_subtitle: Helping managers of RCD and R&amp;D computing teams be more effective
  and less stressed
blog_title: Research Computing Teams
blog_url: https://www.researchcomputingteams.org
category: rct-newsletter
date: '2020-02-07 00:00:00'
layout: post
original_url: https://www.researchcomputingteams.org/newsletter_issues/0005.html
title: '#5 - Link Roundup, Feb 7 2020'
---

<p>Hi!</p>

<p>It’s been an interesting week online for research computing; we have relevant news ranging from managing people to managing workflows at global scale.</p>

<h2 id="managing-teams">Managing Teams</h2>

<p><a href="https://www.labmanager.com/leadership-and-staffing/the-subtle-signs-your-team-is-in-trouble-5407">The Subtle Signs Your Team Is in Trouble</a> - Rachel Muenz, Laboratory Manager<br />
An older article that Laboratory Manager sent around on twitter this week.  It’s written in the context of laboratories, but carries over very clearly to research computing teams.  They mention two opposite signs to look out for:</p>

<ul>
  <li><em>Lack</em> of conflict/disagreement - I think this is a great one, and easy to overlook.  If you have a bunch of technical people in a room and there <strong>isn’t</strong> some steady level of (healthy, respectful) disagreement about how to proceed, then either people are checked out or they don’t feel like they can voice their opinions.  Myself, I try to run tight meetings, which is fine, but I find that unless I’m really careful I can easily make people feel like they should keep disagreements or other “speaking out of turn” to themselves.</li>
  <li>Escalation to open conflict - whether just a learned behaviour or because disagreement has been felt to be tamped down so firmly, when disagreement does happen does it spin out of control?  This is more obvious but still has to be dealt with, and quickly.</li>
</ul>

<p>In both cases the solutions are the same - do lots of constant listening (e.g., through one-on-ones but also just generally) and give lots of feedback (positive and negative; positive for any input even if disagreeing, and positive for doing so constructively while negative for doing so otherwise).</p>

<hr />

<p><a href="https://cloud.google.com/blog/products/management-tools/identifying-and-tracking-toil-using-sre-principles">Tracking Toil using SRE principles</a> - Eric Harvieux, Google Cloud Blog <br />
<a href="https://www.transposit.com/blog/2020.01.30-writing-runbook-documentation-when-youre-an-sre">Writing Runbook Documentation when you’re an SRE</a> - Taylor Barnett, Transposit <br />
“Toil is the kind of work that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows.”</p>

<p>These two articles came out in the same week, and they work very nicely together.  One of the under-appreciated  advantages commercial cloud organizations (or other large operations) have at scale isn’t about hardware - it’s that they have better awareness of how much stuff that gets done is repetitive across teams, and they put the resources in to ensuring those things get done smoothly.</p>

<p>Technical teams tend to hate documenting, but identifying these repetitive “toil” tasks and documenting what’s involved in getting them done in a “runbook” has three really important benefits:</p>

<ul>
  <li>It makes sure the tasks are done reliably in reproducible ways</li>
  <li>It makes it much easier to onboard new people to the team or transfer responsibility for these tasks to someone else</li>
  <li>It is the first step in automating the task to make them easier to do, or ideally to have them done completely automatically.</li>
</ul>

<p>Are there things that have to routinely get done in your team that aren’t documented clearly in (kept-up-to-date) runbooks?  Would things be easier if there were fewer such tasks?</p>

<p>And related to this; here’s a list of “flight rules” (e.g., very short runbooks) for <a href="https://github.com/k88hudson/git-flight-rules">common nontrivial tasks in git</a>.</p>

<hr />

<p>I talked last week about Andy Grove’s book High Output Management, which is aimed at management and in particular senior management in technical organizations; <a href="https://www.g9labs.com/2020/01/04/high-output-management-for-non-managing-tech-leads/">this article</a> talks  about lessons from the book for those still in the trenches, as technical leads (but not managers).</p>

<h2 id="product-management">Product Management</h2>

<p><a href="https://medium.com/ft-product-technology/a-chaos-test-too-far-6405c2cc4bbb">A Chaos Test Too Far</a> - Kathryn Downes &amp; Arjun Gadhia, Financial Times<br />
I’m a big fan of “chaos testing” (or, arguably more appropriate for a lot of research computing work, Slack’s “<a href="https://slack.engineering/disasterpiece-theater-slacks-process-for-approachable-chaos-engineering-3434422afb54">disasterpiece theatre</a>”) for testing the hypotheses that “our system is robust to [bad thing]”; the software equivalent is probably <a href="https://en.wikipedia.org/wiki/Fuzzing">fuzzing</a>.  We know stuff is going to break, usually at the least opportune time; systems crash, users or operators enter invalid information… best to test it in a controlled manner.</p>

<p>So in fairness I should probably post examples of it going - or nearly going - horribly wrong, like this one.  I’d still argue though that the series of events they tested here would almost certainly have happened eventually; far better that it happen with all hands on deck when they were primed to respond rather than towards the end of the day on a lazy Friday afternoon when people were already heading home.</p>

<h2 id="working-with-research-communities">Working with Research Communities</h2>

<p><a href="https://drewdevault.com/2020/01/21/Stress-and-happiness.html">The Happiness and Stresses of Open Source work</a> - Drew Devault<br />
<a href="https://blog.burntsushi.net/foss/">My FOSS story</a> - Andrew Gallant<br /></p>

<p>Research computing teams have a lot in common with open source communities - even if you aren’t developers or developing open source software.   One of the joys of open source communities is that you’re part of a small, visible team solving problems for your users - and that’s exactly the situation we’re in.  But there’s downsides to that, too.  Users can be incredibly demanding, and when you’re a small visible team they can be directly demanding to you, personally.  Members of  huge proprietary teams don’t have that feedback. for good or ill; being one of thousands of devs on MS Word or staff in AWS it can be hard to feel like you’re making a difference, but you’re not getting emails from your users yelling at you because they don’t like that latest change you made.</p>

<p>There’s a lot of talk in FOSS communities these days about dealing with this feedback, including dealing with burnout, and I think this is something that managers should be keeping an eye on.</p>

<h2 id="data-and-infrastructure-tools">Data and Infrastructure Tools</h2>

<p><a href="http://goodtables.io">GoodTables</a><br />
This was talked about <a href="https://fosdem.org/2020/">FOSDEM</a> this year, and will be of interest to some teams and/or of their users: it’s a tool for CI testing of data tables.  It will perform checks of tabular data on push to github or S3.</p>

<p>There were a few other relevant presentations in the <a href="http://FOSDEM 2020 - Databases">databases session</a>, including on <a href="https://dqlite.io">dqlite</a> (simple high-availability distributed database on top of RAFT and sqlite) and <a href="https://nlnet.nl/project/LumoSQL/">LumoSQL</a>, sqlite with faster key-value store and some other advantages.  I find it fascinating how a small high-quality (with hundreds and hundreds of <a href="https://sqlite.org/testing.html">tests</a>!) embedded tool like Sqlite ends up being the basis for so many fun experimental projects.</p>

<hr />

<p><a href="https://www.nextplatform.com/2020/01/31/most-of-hpc-happens-under-the-radar/">Most of HPC Happens Under the Radar</a> - Michael Feldman, Next Platform<br />
This won’t come as a surprise to our community, but it’s an important point worth emphasizing; most HPC doesn’t happen at big centres whose every procurement gets lots of press.</p>

<hr />

<p>Workflow manager news: The <a href="https://www.nature.com/collections/afdejfafdb/">Pan-Cancer Analysis of Whole Genomes</a> results came out this week (see also: <a href="https://news.oicr.on.ca/2020/02/unprecedented-exploration-generates-most-comprehensive-map-of-cancer-genomes-charted-to-date/">Unprecedented exploration generates most comprehensive map of cancer genomes charted to date</a>; <a href="https://insidehpc.com/2020/02/bsc-powers-pan-cancer-project/">BSC Powers Pan-Cancer Project</a>).  Disclaimer: I was somewhat involved in this project.</p>

<p>From a research computing point of view, one of the big accomplishments of this project, particularly given that much of the computational work was done ~five years ago, was coordinating the running of uniform pipelines on ~2800 sets of cancer genomic sequencing data at centres across the globe.   This really helped catalyze work in the genomics community around workflow execution - spurring standards like the <a href="https://www.ga4gh.org/news/ga4gh-wes-api-enables-portable-genomic-analysis/">Workflow Execution Service</a>; repositories of software and workflows like <a href="http://dockstore.org">Dockstore</a> and helping accelerate the development of workflow runners like <a href="http://toil.ucsc-cgl.org">Toil</a>, <a href="https://cromwell.readthedocs.io">Cromwell</a>, and <a href="https://www.nextflow.io">Nextflow</a>.</p>

<p>One of the things that fascinates me is the difference in workflow runners like these ones for research computing - which are really about running large complex executables that would otherwise have run with bash scripts - and evolving ML workflow runners like Lyft’s recently-announced <a href="https://eng.lyft.com/introducing-flyte-cloud-native-machine-learning-and-data-processing-platform-fb2bb3046a59">Flyte</a>.  Are the differences in use cases fundamental, or will the tools start to converge?  One of the things I’d like to do at some point if there’s interest is a bit of a deep dive into workflow managers of all types.</p>

<h2 id="research-software-development">Research Software Development</h2>

<p>In Python, <a href="https://softwaremaniacs.org/blog/2020/02/05/dicts-ordered/en/">dictionaries are now guaranteed to be ordered by insertion</a> - but sets aren’t.  I can absolutely guarantee this is going to cause incredibly hard-to-track-down bugs in research software in the coming couple years.</p>

<p><a href="https://docs.fast.ai/dev/style.html">Jeremy’s Notes on Fast.AI coding style</a><br />
A bracing reminder (ternary operators!  120-character lines!) that there aren’t “correct” coding styles; the purpose is to make sure teams reduce internal barriers to collaboration by picking one style that works for them and sticking with it.</p>

<h2 id="random">Random</h2>

<p>Are you tired of your desktop’s file system working too well to be able to do good robustness testing of software?  Want to mess with a process’s view of system time?  Want stderr output always colorized so you can tell the difference from stdout?  Check out this list of <a href="https://github.com/gaul/awesome-ld-preload/blob/master/README.md">ld-preload</a> hacks.</p>

<p>Finally, a <a href="https://github.com/icyphox/shlide">bash script to do presentations from the terminal</a>.</p>