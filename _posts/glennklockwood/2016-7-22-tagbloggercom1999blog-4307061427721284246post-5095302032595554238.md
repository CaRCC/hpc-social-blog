---
author: Glenn K. Lockwood's Blog
author_tag: glennklockwood
blog_subtitle: Personal thoughts and opinions of a supercomputing enthusiast
blog_title: Glenn K. Lockwood
blog_url: https://glennklockwood.blogspot.com/search/label/hpc
category: glennklockwood
date: '2016-07-22 07:07:00'
layout: post
original_url: https://glennklockwood.blogspot.com/2016/07/basics-of-io-benchmarking.html
slug: basics-of-i-o-benchmarking
title: Basics of I/O Benchmarking
---

<bound method Tag.renderContents of Most people in the supercomputing business are familiar with using FLOPS as a proxy for how fast or capable a supercomputer is.  This measurement, as observed using the <a href="http://www.netlib.org/benchmark/hpl/">High-Performance Linpack (HPL)</a> benchmark, is the basis for the Top500 list.  However, I/O performance is becoming increasingly important as data-intensive computing becomes a driving force in the HPC community, and even though there is no Top500 list for I/O subsystems, the <a href="http://www.nersc.gov/research-and-development/apex/apex-benchmarks/ior/">IOR</a> benchmark has become the <i>de facto</i> standard way to measure the I/O capability for clusters and supercomputers.<br/><br/>Unfortunately, I/O performance tends to be trickier to measure using synthetic benchmarks because of the complexity of the I/O stack that lies between where data is generated (the CPU) to where it'll ultimately be stored (a spinning disk or SSD on a network file system).  In the interests of clarifying some of the confusion that can arise when trying to determine how capable an I/O subsystem really is, let's take a look at some of the specifics of running IOR.<br/><br/><h2>Getting Started with IOR</h2>IOR writes data sequentially with the following parameters:<br/><ul><li><span style="font-family: monospace;">blockSize</span> (<span style="font-family: monospace;">-b</span>)</li><li><span style="font-family: monospace;">transferSize</span> (<span style="font-family: monospace;">-t</span>)</li><li><span style="font-family: monospace;">segmentCount</span> (<span style="font-family: monospace;">-s</span>)</li><li><span style="font-family: monospace;">numTasks</span> (<span style="font-family: monospace;">-n</span>)</li></ul><div>which are best illustrated with a diagram:</div>
<br/><div class="separator" style="clear: both; text-align: center;"><a href="https://2.bp.blogspot.com/-fok4ue8yCiw/V2B-5BCjIlI/AAAAAAAASw0/do7YfsfV8I00b35WAWTeZdiPeWOau_oxwCLcB/s1600/ior-io-pattern.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="212" src="https://2.bp.blogspot.com/-fok4ue8yCiw/V2B-5BCjIlI/AAAAAAAASw0/do7YfsfV8I00b35WAWTeZdiPeWOau_oxwCLcB/s400/ior-io-pattern.png" width="400"/></a></div>
<br/>These four parameters are all you need to get started with IOR.  However, naively running IOR usually gives disappointing results.  For example, if we run a four-node IOR test that writes a total of 16 GiB:<br/><br/><pre>$ mpirun -n 64 ./ior -t 1m -b 16m -s 16<br/>...<br/>access bw(MiB/s) block(KiB) xfer(KiB) open(s)  wr/rd(s) close(s) total(s) iter<br/>------ --------- ---------- --------- -------- -------- -------- -------- ----<br/><span style="background-color: #ffff7f; border-radius: 4px; color: #c7254e;">write  427.36  </span>  16384      1024.00   0.107961 38.34    32.48    38.34    2<br/><span style="background-color: #ffff7f; border-radius: 4px; color: #c7254e;">read   239.08  </span>  16384      1024.00   0.005789 68.53    65.53    68.53    2<br/>remove -         -          -         -        -        -        0.534400 2<br/></pre><div><br/>we can only get a couple hundred megabytes per second out of a Lustre file system that should be capable of a lot more.<br/><br/>Switching from writing to a single-shared file to one file per process using the <code>-F</code> (<code>filePerProcess=1</code>) option changes the performance dramatically:</div>
<div><br/></div>
<pre>$ mpirun -n 64 ./ior -t 1m -b 16m -s 16 -F<br/>...<br/>access bw(MiB/s) block(KiB) xfer(KiB) open(s)  wr/rd(s) close(s) total(s) iter<br/>------ --------- ---------- --------- -------- -------- -------- -------- ----<br/><span style="background-color: #ffff7f; border-radius: 4px; color: #c7254e;">write  33645   </span>  16384      1024.00   0.007693 0.486249 0.195494 0.486972 1<br/><span style="background-color: #ffff7f; border-radius: 4px; color: #c7254e;">read   149473  </span>  16384      1024.00   0.004936 0.108627 0.016479 0.109612 1<br/>remove -         -          -         -        -        -        6.08     1</pre><div><div><br/>This is in large part because letting each MPI process work on its own file cuts out any contention that would arise because of file locking.  </div>
<div><br/></div>
<div>However, the performance difference between our naive test and the file-per-process test is a bit extreme.  In fact, the only way that 146 GB/sec read rate could be achievable on Lustre is if each of the four compute nodes had over 45 GB/sec of network bandwidth to Lustre--that is, a 400 Gbit link on every compute and storage node.<br/><br/></div>
<div><h2>Effect of Page Cache on Benchmarking</h2>What's really happening is that the data being read by IOR isn't actually coming from Lustre; rather, files' contents are already cached, and IOR is able to read them directly out of each compute node's DRAM.  The data wound up getting cached during the write phase of IOR as a result of Linux (and Lustre) using a write-back cache to buffer I/O, so that instead of IOR writing and reading data directly to Lustre, it's actually mostly talking to the memory on each compute node.</div>
<div><br/></div>
<div>To be more specific, although each IOR process thinks it is writing to a file on Lustre and then reading back the contents of that file from Lustre, it is actually</div>
<div></div>
<ol><li>writing data to a copy of the file that is cached in memory.  If there is no copy of the file cached in memory before this write, the parts being modified are loaded into memory first.</li><li>those parts of the file in memory (called "pages") that are now different from what's on Lustre are marked as being "dirty"</li><li>the write() call completes and IOR continues on, even though the written data still hasn't been committed to Lustre</li><li>independent of IOR, the OS kernel continually scans the file cache for files who have been updated in memory but not on Lustre ("dirt pages"), and then commits the cached modifications to Lustre</li><li>dirty pages are declared non-dirty since they are now in sync with what's on disk, but they remain in memory</li></ol>Then when the read phase of IOR follows the write phase, IOR is able to just retrieve the file's contents from memory instead of having to communicate with Lustre over the network.</div>
<div><br/></div>
<div>There are a couple of ways to measure the read performance of the underlying Lustre file system.  The most crude way is to simply write more data than will fit into the total page cache so that by the time the write phase has completed, the beginning of the file has already been evicted from cache.  For example, increasing the number of segments (<span style="font-family: monospace;">-s</span>) to write more data reveals the point at which the nodes' page cache on my test system runs over very clearly:<br/><div><br/><div class="separator" style="clear: both; text-align: center;"></div>
<div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-7M2BLomSgNA/VyZ8L-G_HpI/AAAAAAAALyU/SSQXrYOqJ94V4W61S9-g-UMs90EJ4waewCK4B/s1600/ior-overflowing-cache.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="271" src="https://3.bp.blogspot.com/-7M2BLomSgNA/VyZ8L-G_HpI/AAAAAAAALyU/SSQXrYOqJ94V4W61S9-g-UMs90EJ4waewCK4B/s400/ior-overflowing-cache.png" width="400"/></a></div>
<br/>However, this can make running IOR on systems with a lot of on-node memory take forever.<br/><br/></div>
<div>A better option would be to get the MPI processes on each node to only read data that they didn't write.  For example, on a four-process-per-node test, shifting the mapping of MPI processes to blocks by four makes each node N read the data written by node N-1.<br/><br/><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-AhRMQWdDOxg/VyZ6lH2wl-I/AAAAAAAALyA/nv-EM4OlhX8BHCNX_Bx173Mr7miyBXx-ACK4B/s1600/IOR%2BreorderTasks.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="131" src="https://1.bp.blogspot.com/-AhRMQWdDOxg/VyZ6lH2wl-I/AAAAAAAALyA/nv-EM4OlhX8BHCNX_Bx173Mr7miyBXx-ACK4B/s400/IOR%2BreorderTasks.png" width="400"/></a></div>
<br/></div>
<div></div>
<div>Since page cache is not shared between compute nodes, shifting tasks this way ensures that each MPI process is reading data it did not write.</div>
<div><br/>IOR provides the <span style="font-family: monospace;">-C</span> option (reorderTasks) to do this, and it forces each MPI process to read the data written by its neighboring node.  Running IOR with this option gives much more credible read performance:</div>
<div><br/></div>
<pre>$ mpirun -n 64 ./ior -t 1m -b 16m -s 16 -F -C<br/>...<br/>access bw(MiB/s) block(KiB) xfer(KiB) open(s)  wr/rd(s) close(s) total(s) iter<br/>------ --------- ---------- --------- -------- -------- -------- -------- ----<br/><span style="background-color: #ffff7f; border-radius: 4px; color: #c7254e;">write  41326   </span>  16384      1024.00   0.005756 0.395859 0.095360 0.396453 0<br/><span style="background-color: #ffff7f; border-radius: 4px; color: #c7254e;">read   3310.00 </span>  16384      1024.00   0.011786 4.95     4.20     4.95     1<br/>remove -         -          -         -        -        -        0.237291 1<br/></pre><br/>But now it should seem obvious that the write performance is also ridiculously high.  And again, this is due to the page cache, which signals to IOR that writes are complete when they have been committed to memory rather than the underlying Lustre file system.<br/><br/>To work around the effects of the page cache on write performance, we can issue an <span style="font-family: monospace;">fsync()</span> call immediately after all of the <span style="font-family: monospace;">write()</span>s return to force the dirty pages we just wrote to flush out to Lustre.  Including the time it takes for <span style="font-family: monospace;">fsync()</span> to finish gives us a measure of how long it takes for our data to write to the page cache and for the page cache to write back to Lustre.<br/><br/>IOR provides another convenient option, <span style="font-family: monospace;">-e</span> (<span style="font-family: monospace;">fsync</span>), to do just this.  And, once again, using this option changes our performance measurement quite a bit:<br/><br/></div>
<pre>$ mpirun -n 64 ./ior -t 1m -b 16m -s 16 -F -C -e<br/>...<br/>access bw(MiB/s) block(KiB) xfer(KiB) open(s)  wr/rd(s) close(s) total(s) iter<br/>------ --------- ---------- --------- -------- -------- -------- -------- ----<br/><span style="background-color: #ffff7f; border-radius: 4px; color: #c7254e;">write  2937.89 </span>  16384      1024.00   0.011841 5.56     4.93     5.58     0<br/><span style="background-color: #ffff7f; border-radius: 4px; color: #c7254e;">read   2712.55 </span>  16384      1024.00   0.005214 6.04     5.08     6.04     3<br/>remove -         -          -         -        -        -        0.037706 0</pre><br/>and we finally have a believable bandwidth measurement for our file system. <br/><br/><h2>Defeating Page Cache</h2>Since IOR is specifically designed to benchmark I/O, it provides these options that make it as easy as possible to ensure that you are actually measuring the performance of your file system and not your compute nodes' memory.  That being said, the I/O patterns it generates are designed to demonstrate peak performance, not reflect what a real application might be trying to do, and as a result, there are plenty of cases where measuring I/O performance with IOR is not always the best choice.  There are several ways in which we can get clever and defeat page cache in a more general sense to get meaningful performance numbers.<br/><br/>When measuring <b>write performance</b>, bypassing page cache is actually quite simple; opening a file with the <span style="font-family: monospace;">O_DIRECT</span> flag going directly to disk.  In addition, the <span style="font-family: monospace;">fsync()</span> call can be inserted into applications, as is done with IOR's <span style="font-family: monospace;">-e</span> option.<br/><br/>Measuring <b>read performance</b> is a lot trickier.  If you are fortunate enough to have root access on a test system, you can force the Linux kernel to empty out its page cache by doing<br/><blockquote class="tr_bq"><span style="font-family: monospace;"># echo 1 &gt; /proc/sys/vm/drop_caches</span></blockquote>and in fact, this is often good practice before running any benchmark (e.g., Linpack) because it ensures that you aren't losing performance to the kernel trying to evict pages as your benchmark application starts allocating memory for its own use.<br/><br/>Unfortunately, many of us do not have root on our systems, so we have to get even more clever.  As it turns out, there is a way to pass a hint to the kernel that a file is no longer needed in page cache:<br/><br/><br/>The effect of passing <span style="font-family: monospace;">POSIX_FADV_DONTNEED</span> using <span style="font-family: monospace;">posix_fadvise()</span> is usually that all pages belonging to that file are evicted from page cache in Linux.  However, this is just a hint--not a guarantee--and the kernel evicts these pages asynchronously, so it may take a second or two for pages to actually leave page cache.  Fortunately, Linux also provides a way to <a href="https://github.com/glennklockwood/atgtools/blob/master/is_file_in_page_cache.c">probe pages in a file to see if they are resident in memory</a>.<br/><br/>Finally, it's often easiest to just limit the amount of memory available for page cache.  Because application memory always takes precedence over cache memory, simply allocating most of the memory on a node will force most of the cached pages to be evicted.  Newer versions of IOR provide the <span style="font-family: monospace;">memoryPerNode</span> option that do just that, and the effects are what one would expect:<br/><br/><div class="separator" style="clear: both; text-align: center;"><a href="https://1.bp.blogspot.com/-xiC1K4absXU/V5GVEAfe5dI/AAAAAAAAgwY/HyO4J_ORd2gnJLF7aD3JpNu9p9MqjOc-ACLcB/s1600/ior-memPerNode-test.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="271" src="https://1.bp.blogspot.com/-xiC1K4absXU/V5GVEAfe5dI/AAAAAAAAgwY/HyO4J_ORd2gnJLF7aD3JpNu9p9MqjOc-ACLcB/s400/ior-memPerNode-test.png" width="400"/></a></div>
<br/>The above diagram shows the measured bandwidth from a single node with 128 GiB of total DRAM.  The first percent on each x-label is the amount of this 128 GiB that was reserved by the benchmark as application memory, and the second percent is the total write volume.  For example, the "50%/150%" data points correspond to 50% of the node memory (64 GiB) being allocated for the application, and a total of 192 GiB of data being read.<br/><br/>This benchmark was run on a single spinning disk which is not capable of more than 130 MB/sec, so the conditions that showed performance higher than this were benefiting from some pages being served from cache.  And this makes perfect sense given that the anomalously high performance measurements were obtained when there was plenty of memory to cache relative to the amount of data being read.<br/><br/><h2>Corollary </h2>Measuring I/O performance is a bit trickier than CPU performance in large part due to the effects of page caching.  That being said, page cache exists for a reason, and there are many cases where an application's I/O performance really is best represented by a benchmark that heavily utilizes cache.<br/><br/>For example, the BLAST bioinformatics application re-reads all of its input data twice; the first time initializes data structures, and the second time fills them up.  Because the first read caches each page and allows the second read to come out of cache rather than the file system, running this I/O pattern with page cache disabled causes it to be about 2x slower:<br/><br/><div class="separator" style="clear: both; text-align: center;"><a href="https://4.bp.blogspot.com/-KBZ0TDtNz5w/V5Gc8XLAS3I/AAAAAAAAgwo/GWH6i3xp98oSHilPgPAipG75cClgDhkuACLcB/s1600/cache-vs-nocache.png" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="290" src="https://4.bp.blogspot.com/-KBZ0TDtNz5w/V5Gc8XLAS3I/AAAAAAAAgwo/GWH6i3xp98oSHilPgPAipG75cClgDhkuACLcB/s400/cache-vs-nocache.png" width="400"/></a></div>
<br/>Thus, letting the page cache do its thing is often the most realistic way to benchmark with realistic application I/O patterns.  Once you know <i>how </i>page cache might be affecting your measurements, you stand a good chance of being able to reason about what the most meaningful performance metrics are.>