---
author: Glenn K. Lockwood's Blog
author_tag: glennklockwood
blog_subtitle: Personal thoughts and opinions of a supercomputing enthusiast
blog_title: Glenn K. Lockwood
blog_url: https://glennklockwood.blogspot.com/search/label/hpc
category: glennklockwood
date: '2014-02-12 20:20:00'
layout: post
original_url: https://glennklockwood.blogspot.com/2014/02/linux-perf-libquadmath-and-gfortrans.html
slug: linux-perf-libquadmath-and-gfortran-s-insane-behavior
title: Linux perf, libquadmath, and GFortran's Insane Behavior
---

<bound method Tag.renderContents of <b>Executive Summary</b>: <a href="http://gcc.gnu.org/onlinedocs/libquadmath/">libquadmath</a> was introduced in GFortran 4.6 which fundamentally changed what the <span style="font-family: Courier New, Courier, monospace;">-fdefault-real-8</span> switch does.  Rather than promoting all floating point arithmetic to double precision, it doubles the width of all floating point types, so explicitly typed double precision is converted to quad precision.  This quad precision is orders of magnitude slower since it must be done in software, causing binaries built with <span style="font-family: Courier New, Courier, monospace;">-fdefault-real-8</span> to grind to a halt when built with GFortran 4.6 and newer.  The solution is to add <span style="font-family: Courier New, Courier, monospace;">-fdefault-double-8</span> to undo this implicit doubling of explicit <span style="font-family: Courier New, Courier, monospace;">real*8</span>.<br/><br/>What follows is a case study of sorts in how I discovered this.  Maybe my methodology will be useful for others who are tasked with debugging performance problems.<br/><br/><h2 id="prob">The Problem</h2>A colleague from my past in research science sent me an e-mail this morning with a very typical problem that people run into whenever they try transferring their applications from one machine to another.  He wrote,<br/><blockquote class="tr_bq">"I've been having a problem with compiling on a new workstation that is an HP with the newer gcc/gfortran 4.6.3.  The executable for the code runs very slow.  If I compile the exact same on the cluster or one of the Dell workstations (both have gfortran 4.4.3) it runs very fast on both.  Also, if I transfer the compiled binary from the cluster to the new HP workstation, it runs fast."</blockquote>That is to say,<br/><br/><table style="border-collapse: collapse; border: 1px solid black; margin: 0 auto; text-align: center;"><tbody><tr style="border: 1px solid black;"><th></th><th style="border: 1px solid black;">Run on New <br/>Workstation</th><th style="border: 1px solid black;">Run on Old<br/>Workstation</th></tr><tr style="border: 1px solid black;"><th style="border: 1px solid black; text-align: right;">Compiled on <br/>New Workstation</th><td style="background: #ff7777; border: 1px solid black;">SLOW</td><td style="border: 1px solid black;">?</td></tr><tr><th style="text-align: right;">Compiled on <br/>Old Workstation</th><td style="background: #77ff77; border: 1px solid black;">FAST</td><td style="background: #77ff77;">FAST</td></tr></tbody></table><br/><div>The fact that the old binary ran fast on the new machine ruled out some odd hardware or kernel problem and suggested that the issue was somewhere in userland.  Userland issues are always fixable issues, so this alone suggests that there will be a solution to this issue if we dig deep enough.</div>
<div><br/></div>
<h2 id="arcane">A Little Logic, A Little Arcane Knowledge</h2><div>The difference in performance was probably related to the upgrade from GFortran 4.4 to GFortran 4.6, and just to make sure this was a well-defined problem, I re-built the application and ran the test case on a local machine to ensure that the problem was reproducible on hardware and an OS with which I was familiar.  I built with</div>
<div><ul><li>The GFortran 4.4 that ships with Red Hat 6.  My colleague said that his build with GFortran 4.4 ran fine, and I was able to confirm that <b>GFortran 4.4 produced a reliably fast executable</b>.</li><li>The GFortran 4.6 that ships with Ubuntu 12.04 (my colleague's machine).  He said that this one ran very slowly, and I could confirm that <b>GFortran 4.6 did, indeed, produce an unusably slow binary</b>. </li><li>The GFortran 4.8 that I built as a "latest-and-greatest" version on my test system.  I wanted to verify that there wasn't some bug in 4.6 that was patched out of subsequent releases.  Unfortunately this was not the case, as <b>GFortran 4.8 also produced a very slow binary</b>.</li></ul><div>The good news was that the problem is reproducible and we have a baseline case where the application <i>does</i> behave as intended.  This meant that, in the worst-case scenario, we can do line-by-line comparisons of the assembly code for the working and non-working binaries to see where the problem lies.  Thus, we know the problem has a solution.</div>
<div><br/></div>
<div>Of course, the bad news was that some change made between GFortran 4.4 and GFortran 4.6 broke this code, and we have to figure out exactly what this change was.</div>
<div><br/></div>
<div>This is where arcane knowledge comes in: I know two facts about GCC that suggest this may be, in fact, a problem with GFortran:</div>
</div>
<div><ol><li>GFortran has been known to throw backwards compatibility to the wind and make wild changes default behavior.  For example, g77 and GFortran 4.1 used 8-byte record marker lengths by default, but then <a href="http://gcc.gnu.org/wiki/GFortran/News#gfortran_4.2">switched over to 4-byte markers in GFortran 4.2 to be in line with what every other Fortran compiler does</a>.  This meant that data generated by GFortran 4.1 was not compatible with anything else.  It wouldn't have surprised me if they did this sort of thing again.</li><li><a href="http://gcc.gnu.org/wiki/GFortran/News#gfortran_4.6">GCC introduced libquadmath in version 4.6</a> which made all GFortran objects built with 4.6 or later pull in libquadmath.  This used to cause me problems because Red Hat 5 did not ship with libquadmath, making all binaries dynamically linked against GFortran 4.6 not portable* to RHEL5.  Thus, this issue might have something to do with the addition of libquadmath.</li></ol><div><span style="font-size: xx-small;">* I acknowledge that trying to move binaries between machines is pretty crazy in its own right.  Explaining why this was an actual issue for me is both uninteresting and beyond the scope of this post.</span><br/><br/></div>
<h2 id="perfuse">Examining Baseline Performance</h2></div>
<div>All modern Linux kernels ship with the <a href="https://perf.wiki.kernel.org/">perf subsystem</a> which makes diagnosing performance problems significantly easier than it has been in the past.  If you haven't familiarized yourself with them yet, you really need to--all it took for me was a 2-minute demo by <a class="g-profile" href="https://plus.google.com/109775321689856324025" target="_blank">+Peter Kjellström</a> at SC'13 last year to realize Linux perf is serious business.  We will simply use it as an alternative to gprof in this case so that we don't have to re-build all this code with instrumentation, but <a href="https://perf.wiki.kernel.org/index.php/Tutorial">perf can also do a lot of things</a> that used to be the exclusive domain of special-purpose libraries like <a href="http://icl.cs.utk.edu/papi/">PAPI</a> and <a href="http://ipm-hpc.org/">IPM</a>.<br/><br/>Running the "good" build of this application through<sup>†</sup> perf establishes our baseline expected behavior:<br/><br/><pre style="font-family: monospace; font-size: smaller; margin-left: 2em;">$ <span style="color: #0b5394;"><b>perf record -o fast.report -g ./mdvgg.x</b></span><br/>WARNING: Kernel address maps (/proc/{kallsyms,modules}) are restricted,<br/>check /proc/sys/kernel/kptr_restrict.<br/><br/>Samples in kernel functions may not be resolved if a suitable vmlinux<br/>file is not found in the buildid cache or in the vmlinux path.<br/><br/>Samples in kernel modules won't be resolved at all.<br/><br/>If some relocation was applied (e.g. kexec) symbols may be misresolved<br/>even with a suitable vmlinux or kallsyms file.<br/><br/><span style="color: #999999;"> Pressure list found<br/> taux,y,z:         0.000000       0.000000       0.000000<br/> txx0,tyy0,tzz0:   1013250.       1013250.       1013250.<br/> Wolf beta value:  4.4600E-008<br/><br/>***Lennard-Jones parameters, epsilons in ergs ***                               <br/> 0.00000D+00    0.00000D+00    0.00000D+00    0.00000D+00    0.00000D+00<br/>...<br/>  average energy per atom:        -0.39932E-11    -57.4739kcal/mole<br/>  average energy with selfterm:   -0.51864E-11    -74.6481kcal/mole</span><br/>[ perf record: Woken up 3 times to write data ]<br/>[ perf record: Captured and wrote 0.895 MB fast.report (~39121 samples) ]<br/></pre>where<br/><ul><li><span style="font-family: Courier New, Courier, monospace;">-o fast.report</span> dumps the recorded data to a file called <span style="font-family: Courier New, Courier, monospace;">fast.report</span></li><li><span style="font-family: Courier New, Courier, monospace;">-g</span> generates call graphs in addition to the flat profile (this isn't always necessary)</li><li><span style="font-family: Courier New, Courier, monospace;">./mdvgg.x</span> is the application binary we are profiling</li></ul><br/>The scary warnings about kernel functions are harmless and a result of this entire debugging process being run as an unprivileged user.  Once the job finishes running, viewing the report reveals (with some extraneous data removed for brevity):<br/><br/><pre style="font-family: monospace; font-size: smaller; margin-left: 2em;">$ <span style="color: #0b5394;">perf report -i fast.report --stdio --sort dso -g flat</span><br/>...</pre><pre style="font-family: monospace; font-size: smaller; margin-left: 2em;"># Overhead  Command         Shared Object<br/># ........  .......  ....................<br/>#<br/>    72.13%  mdvgg.x  mdvgg.x             <br/>            61.12%<br/>                <b><span style="color: red;">pairs_</span></b><br/>                <b><span style="color: blue;">move1_</span></b><br/><br/>             7.00%<br/>                listwater_<br/>                bulk_<br/>                MAIN__<br/>                0x400efd<br/>...<br/>    20.99%  mdvgg.x  libc-2.12.so        <br/>            14.09%<br/>                <b><span style="color: magenta;">__memset_sse2</span></b><br/>                bulk_<br/>                MAIN__<br/>                0x400efd<br/><br/>             0.97%<br/>                <b><span style="color: magenta;">__memset_sse2</span></b><br/>                MAIN__<br/>                0x400efd<br/></pre>where<br/><ul><li><span style="font-family: Courier New, Courier, monospace;">-i fast.report</span> is the file containing our recorded data</li><li><span style="font-family: Courier New, Courier, monospace;">--stdio</span> prevents perf from using the interactive text user interface (I only added this because I can't paste interactions into a blog)</li><li><span style="font-family: Courier New, Courier, monospace;">--sort dso</span> presents the output in a relatively compact way sorted by the shared object in which time was being spent</li><li><span style="font-family: Courier New, Courier, monospace;">-g flat</span> presents a relatively flat profile (we don't need the full call graph)</li></ul><br/>Thus, the majority of our runtime is taken up in a subroutine called <span style="color: red; font-family: Courier New, Courier, monospace;"><b>pairs</b></span>, called from <span style="color: blue; font-family: Courier New, Courier, monospace;"><b>move1</b></span> when this application is working normally.  A surprising fraction of runtime was also consumed by <span style="color: magenta; font-family: Courier New, Courier, monospace;"><b>memset(3)</b></span> in this case, but this was the result of my test input being so small that most of the actual runtime was spent doing initialization.  Even though this is generally not a great way to test application performance, it is acceptable in this case because even initialization takes 20x longer with the "bad" binary built against GFortran 4.6 <span style="font-size: xx-small;">(which in itself is a very insightful behavior that suggests that there is something systematically wrong with the bad binary)</span>.  The simplest and shortest possible run required to reproduce the issue should elucidate where the problem lies.<br/><br/>Now, profiling the "bad" binary built with GFortran 4.6 should give us a definite place to start looking:<br/><br/><pre style="font-family: monospace; font-size: smaller; margin-left: 2em;">$ <span style="color: #0b5394;"><b>perf record -o slow.report -g ./mdvgg.x</b></span><br/>WARNING: Kernel address maps (/proc/{kallsyms,modules}) are restricted,<br/>check /proc/sys/kernel/kptr_restrict.<br/>...<br/><br/>$ <span style="color: #0b5394;"><b>perf report -i slow.report --stdio --sort dso -g flat</b></span><br/>...<br/># Overhead         Shared Object<br/># ........  ....................<br/>#<br/>    93.59%  libgcc_s.so.1       <br/>            48.69%<br/>                <b><span style="color: blue;">__sfp_handle_exceptions</span></b><br/><br/>            13.54%<br/>                <b><span style="color: magenta;">__multf3</span></b><br/><br/>             6.89%<br/>                <b><span style="color: magenta;">__addtf3</span></b><br/><br/>             6.16%<br/>                <b><span style="color: magenta;">__subtf3</span></b><br/><br/>...<br/>     3.02%  libquadmath.so.0.0.0<br/>             1.62%<br/>                __sfp_handle_exceptions<br/><br/>     <b><span style="color: red;">2.67%  mdvgg.x</span></b>             <br/>             1.91%<br/>                hcristo_<br/>                fcc100_<br/><br/>...</pre><br/>Well there's our problem!  Only <b><span style="color: red;">2.67%</span></b> of the application runtime is actually being spent running the <span style="font-family: Courier New, Courier, monospace;">mdvgg.x</span> application, and a huge amount of time is being spent in some <span style="color: blue; font-family: Courier New, Courier, monospace;"><b>__sfp_handle_exceptions</b></span> call.  What gives?<br/><br/>Now I'm not ashamed to say that I routinely turn to Google to figure out what most of this sort of computer nonsense means.  Unfortunately, searching for "<span style="font-family: Courier New, Courier, monospace;">__sfp_handle_exceptions</span>" doesn't turn up anything useful, so the only hint we have is that the name of the call suggests that this "bad" build is generating a lot of floating point exceptions (FPEs). <br/><br/>The logical next step is to rebuild the application with a lot of FPE trapping (<span style="font-family: Courier New, Courier, monospace;">FCFLAGS+=-ffpe-trap=invalid,zero,overflow,underflow,denormal</span>).  This will determine if the code had been generating a ton of floating point exceptions all along but GFortran had just gotten stricter in 4.6.  Unfortunately, doing this just leads to more disappointment--the application does not generate any of the common floating point exceptions, meaning that this mysterious <span style="font-family: Courier New, Courier, monospace;">__sfp_handle_exceptions</span> is, in fact, not handling serious floating point exceptions.  What else could it be doing?<br/><br/><span style="font-size: xx-small;"><sup>†</sup> Although this particular application was both quick enough to run entirely through perf and serial enough to not require any special considerations with MPI, getting these perf profiles from long-running and highly parallel codes is similarly easy.  Instead of running the application through perf (<span style="font-family: Courier New, Courier, monospace;">perf record -o fast.report -g ./mdvgg.x</span>) you can attach perf to an already-running process for a fixed period of time to generate a sample of the overall performance profile.  This is achieved by doing <span style="font-family: Courier New, Courier, monospace;">perf record -o fast.report -g -p <i></i> sleep 10</span>.  Perf attaches to the specified pid and gathers data from it, and just sleeps for ten seconds before detaching.</span><br/><br/><h2 id="diagnosis">Quad-Precision: Back to the Intel 80286</h2>Giving up on <span style="font-family: Courier New, Courier, monospace;">__sftp_handle_exceptions</span> and moving on down the performance profile, it appears that suddenly libquadmath (which, as I mentioned above, appeared after our "working" compiler version was released) is soaking up cycles.  Furthermore, a quick googling of some of those big offenders like <span style="color: magenta; font-family: Courier New, Courier, monospace;"><b>__multf3</b></span>, <span style="color: magenta; font-family: Courier New, Courier, monospace;"><b>__addtf3</b></span>, and <span style="color: magenta; font-family: Courier New, Courier, monospace;"><b>__subtf3</b></span> reveals that <b>they are software implementations of long-double arithmetic</b>--the application is now doing quad precision arithmetic in this "bad" build whereas it is definitely not doing this in our "good" build.<br/><br/>Suddenly everything becomes a little clearer: long-double floating point arithmetic involves numbers stored in 128-bit precision, but 64-bit CPUs <span style="font-size: xx-small;">(or more properly, FPUs)</span> are only capable of handling (you guessed it) 64-bit precision floating point calculations.  Thus, to get an application to do calculations in 128-bit precision, a software layer (libquadmath) must emulate 128-bit floating point hardware and actually translate the binary logic into something the 64-bit CPU can understand.  This is analogous to getting a 3rd grader to do a large calculation (e.g., 6×8) by breaking into pieces they know how to solve (e.g., 8+8, 8+8, 8+8), and it is a <i>very</i> slow process.  This massive performance loss is why Intel has had a hardware floating point unit in every processor it's designed since the 20386 (ca. 1985).<br/><br/>The obvious question is then why GFortran 4.6 has decided to start carrying out all of the calculations in this code in quad precision by default.  Surely the GFortran developers didn't think forcing all arithmetic to be done in software was a good idea, right?<br/><br/><h2 id="gf">Redefining Default Behavior</h2>Of course not. <br/><br/>The next challenge, then, is to dig through the GFortran 4.6 manual to figure out what the libquadmath integration did to default behavior, or alternatively, what compiler flags started changing the precision of variables and calculations automatically.<br/><br/>This is where knowledge of Fortran becomes important, because an unfortunate aspect of F77 (which has carried forward in F90) is its implicit typing.  A novice Fortran programmer (like a new graduate student) may think that doing something like<br/><br/><pre style="margin-left: 2em;">implicit real*8(a-h,o-z)<br/>value1 = 0.31415926535e+1</pre><br/>will store a double-precision (<span style="font-family: Courier New, Courier, monospace;">real*8</span>) value in <span style="font-family: Courier New, Courier, monospace;">value1</span>.  This isn't the case, as the "<span style="font-family: Courier New, Courier, monospace;">e+1</span>" instead of "<span style="font-family: Courier New, Courier, monospace;">d+1</span>" tends to render this a single-precision value.  This isn't <i>always</i> the case, but let it suffice to say that the details get messy and I've seen different compilers handle this in different ways by default.<br/><br/>Anyway, every Fortran compiler has options to override this implicit typing and force all floating point values into double precision.  In GFortran, this has traditionally been <span style="font-family: Courier New, Courier, monospace;">-fdefault-real-8</span>; that is, the default data type for real (i.e., single-precision) values is <span style="font-family: Courier New, Courier, monospace;">real*8</span>, or double precision.  In this particular code's makefile, this flag was enabled to override the sloppy coding practices of decades of graduate students and ensure precision wasn't going down the drain because someone used E's instead of D's in 1997.<br/><br/>When a simple search for "quadmath" in the GFortran 4.6 manual turns up nothing, searching for <span style="font-family: Courier New, Courier, monospace;">-fdefault-real-8</span> is the next step.  Lo and behold, <a href="http://gcc.gnu.org/onlinedocs/gcc-4.6.3/gfortran/Fortran-Dialect-Options.html">this gem appears</a>:<br/><blockquote class="tr_bq"><span style="font-family: Courier New, Courier, monospace;">-fdefault-real-8</span><br/>Set the default real type to an 8 byte wide type. Do nothing if this is already the default. This option also affects the kind of non-double real constants like 1.0, and <b>does promote the default width of DOUBLE PRECISION to 16 bytes if possible</b>, unless <span style="font-family: Courier New, Courier, monospace;">-fdefault-double-8</span> is given, too. </blockquote>Bingo.  Any code that previously used <span style="font-family: Courier New, Courier, monospace;">-fdefault-real-8</span> to ensure that all floating point arithmetic was being done in double precision now does <b>all explicitly typed double precision arithmetic as 128-bit quad precision in software as its effective default behavior</b>.  What's worse is that this change in behavior <a href="http://gcc.gnu.org/gcc-4.6/changes.html#fortran">isn't even mentioned in the release notes for GFortran 4.6</a> because <span style="font-family: Courier New, Courier, monospace;">-fdefault-real-8</span> has always <i>tried</i> to promote <span style="font-family: Courier New, Courier, monospace;">real*8</span> to <span style="font-family: Courier New, Courier, monospace;">real*16</span> as its intended behavior; it simply never succeeded because GFortran didn't support software quad-precision before libquadmath appeared in 4.6.<br/><br/>Quite frankly, defining the behavior of something as straightforward-sounding as <span style="font-family: Courier New, Courier, monospace;">-fdefault-real-8</span> to be so environment-specific is insane.  The only use case where this new behavior would even make sense is if a programmer intentionally mixes <span style="font-family: Courier New, Courier, monospace;">real*4</span> and <span style="font-family: Courier New, Courier, monospace;">real*8</span> datatypes within code and wants to see what will happen if all variable widths are doubled uniformly.  On the other hand if <span style="font-family: Courier New, Courier, monospace;">-fdefault-real-8</span> was being used to ensure all calculations were done in double-precision (as was the case in this application and at least a few other unrelated scientific codes with which I have worked), performance takes a catastrophic hit simply because a new quad-precision math library is bundled with GCC.<br/><br/>It would make more sense if GFortran added a <span style="font-family: Courier New, Courier, monospace;">-fdefault-real-16</span> (a la Intel Fortran's <span style="font-family: Courier New, Courier, monospace;">-real-size 128</span> switch) to promote all floating point to quad precision.  In fact, I find it difficult to make sense of GFortran's choice to make <span style="font-family: Courier New, Courier, monospace;">-fdefault-real-8</span> preserve mixed precision codes as it does; the only case where I can envision this sort of behavior being useful is in codes that implement their own reduced-precision FFTs.  I have literally never encountered such a code, though.<br/><br/>Ultimately the solution to this problem, for those who are fortunate enough to get to the bottom of it, is to simply add <span style="font-family: Courier New, Courier, monospace;">-fdefault-double-8</span> in addition to <span style="font-family: Courier New, Courier, monospace;">-fdefault-real-8</span>.  This was enough to fix the issue my colleague was having, and now his lab is back to crunching away with molecular dynamics at normal speed.</div>
>