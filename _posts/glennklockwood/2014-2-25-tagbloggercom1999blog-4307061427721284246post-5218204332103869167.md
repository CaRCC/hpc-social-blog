---
author: Glenn K. Lockwood's Blog
author_tag: glennklockwood
blog_subtitle: Personal thoughts and opinions of a supercomputing enthusiast
blog_title: Glenn K. Lockwood
blog_url: https://glennklockwood.blogspot.com/search/label/hpc
category: glennklockwood
date: '2014-02-25 16:42:00'
layout: post
original_url: https://glennklockwood.blogspot.com/2014/02/quantum-espresso-performance-benefits.html
slug: quantum-espresso-performance-benefits-of-vendor-optimized-libraries
title: Quantum ESPRESSO- Performance Benefits of Vendor-Optimized Libraries
---

<bound method Tag.renderContents of <div class="p1">In my previous post, I presented a lot of different options you can use to build Quantum ESPRESSO which are (admittedly) very confusing.  At the end of the day, the set of options that produce the fastest-running executable matters the most, so I went through and benchmarked many of the permutations of compiler/MPI/library options.</div>
<div class="p1"><br/>What this post ultimately illustrates is that <i>you should never use the Netlib reference implementations of BLAS and LAPACK</i>; even <a href="http://www.netlib.org/blas/faq.html#5">Netlib says as much</a>.  ScaLAPACK is much less broadly supported by hardware vendors (e.g., the ACML library that shipped with the PGI compiler I used did not include it), but most of the hardware-dependent optimizations are done below the BLACS level and within the MPI library and associated hardware drivers.  As such, I was able to use Intel's MKL ScaLAPACK when building with the Intel compiler in the data below, but I had to use Netlib's ScaLAPACK with ACML-optimized BLAS and LAPACK when compiling with PGI.<br/><br/>The actual benchmark I used was the <a href="http://www.deisa.eu/science/benchmarking/codes/quantumespresso">DEISA AUSURF112 benchmark</a> problem with only one pool using 64 MPI processes.  The two testing platforms were<br/><br/><ul><li>SDSC's Gordon supercomputer (four nodes)</li><ul><li>16× 2.6 GHz Intel Xeon E5-2670 (Sandy Bridge) cores</li><li>64 GB DDR3 SDRAM</li><li>Mellanox ConnectX-3 QDR HCAs on PCIe 3.0</li><li>Mellanox Infiniscale IV switch</li></ul><li>SDSC's Trestles supercomputer (two nodes)</li><ul><li>32× 2.4 GHz AMD Opteron 6136 (Magny Cours) nodes</li><li>64 GB DDR3 SDRAM</li><li>Mellanox ConnectX QDR HCAs on PCIe 2.0</li><li>Voltaire Grid Director 4700 switch</li></ul></ul><br/>I don't know the port-to-port latency for the Trestles runs, but the application is bandwidth-bound due to the problem geometry (one pool) and the large amount of <span style="font-family: Courier New, Courier, monospace;">MPI_Allreduce</span>s and <span style="font-family: Courier New, Courier, monospace;">MPI_Alltoallv</span>s render the latency largely irrelevant.  More information about the communication patterns of this benchmark are available from the <a href="http://www.hpcadvisorycouncil.com/pdf/QuantumEspresso_Performance_Analysis.pdf">HPC Advisory Council</a>.<br/><br/>On both testing systems, the software versions were the same:<br/><ul><li><b>Compilers</b>: Intel 2013.1.117 and PGI 13.2</li><li><b>MPI libraries</b>: MVAPICH2 1.9 and OpenMPI 1.6.5</li><li><b>Vendor FFTs</b>: MKL 11.0.1 and ACML 5.3.0</li><li><b>Vendor BLAS/LAPACK</b>: MKL 11.0.1 and ACML 5.3.0</li><li><b>Vendor ScaLAPACK</b>: MKL 11.0.1 (used Netlib ScaLAPACK 2.0.2 with PGI)</li><li><b>Reference FFTs</b>: FFTW 3.3.3</li><li><b>Reference BLAS/LAPACK</b>: Netlib 3.4.2</li><li><b>Reference ScaLAPACK</b>: Netlib 2.0.2</li></ul><br/><h2>Vendor-optimized Libraries</h2>On Gordon, MKL shows extremely good performance compared to ACML, and this is to be expected given the fact that Intel's MKL is optimized for Gordon's ability to do AVX operations.<br/><br/><div class="separator" style="clear: both; text-align: center;"></div>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-9W-NxWoL_-c/UwASBxQ_ZBI/AAAAAAAAKRc/wN5-WgL4y7Q/s1600/Gordon+Comparison.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="271" src="http://3.bp.blogspot.com/-9W-NxWoL_-c/UwASBxQ_ZBI/AAAAAAAAKRc/wN5-WgL4y7Q/s1600/Gordon+Comparison.png" width="400"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Performance with vendor libraries on Gordon</td></tr></tbody></table></div>
<div class="p1"><br/>In addition, the difference in MPI libraries is also quite consistent.  Although the point-to-point performance of MVAPICH2 and OpenMPI over the same fabric should be comparable, the two libraries have different implementations of MPI collective operations.  Quantum ESPRESSO is dominated by costly <span style="font-family: Courier New, Courier, monospace;">MPI_Allreduce</span> and <span style="font-family: Courier New, Courier, monospace;">MPI_Alltoallv</span>, so the level of optimization within the MPI implementations is very apparent.<br/><br/>In fact, the PGI and OpenMPI build (which uses the Netlib ScaLAPACK, as opposed to a vendor-supplied ScaLAPACK which MKL provides) would hang on collectives unless the following environment variable was passed to the OpenMPI runtime:<br/><br/><pre>OMPI_MCA_coll_sync_barrier_after=100</pre><br/>This switch forces the OpenMPI runtime to sync all processes after every 100 collective operations to prevent certain MPI ranks from racing so far ahead of the rest that a deadlock occurs.  OpenMPI does this after every 1,000 collectives by default.  Alternatively, HPCAC suggests the following tunings for OpenMPI:<br/><br/><pre>OMPI_MCA_mpi_affinity_alone=1<br/>OMPI_MCA_coll_tuned_use_dynamic_rules=1<br/>OMPI_MCA_coll_tuned_barrier_algorithm=6<br/>OMPI_MCA_coll_tuned_allreduce_algorithm=0</pre><br/>These collective tunings also prevented deadlocking of the benchmark, but the performance was no better than simply increasing the implicit barrier frequency with <span style="font-family: Courier New, Courier, monospace;">OMPI_MCA_coll_sync_barrier_</span>*.<br/><br/>Trestles, with its AMD processors, does not realize as large a benefit from using MKL:<br/><br/></div>
<div class="p1"><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-ijpV8iYr6uw/UwASP0zFqQI/AAAAAAAAKRk/peFbho9NXQ0/s1600/Trestles+Comparison.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="271" src="http://3.bp.blogspot.com/-ijpV8iYr6uw/UwASP0zFqQI/AAAAAAAAKRk/peFbho9NXQ0/s1600/Trestles+Comparison.png" width="400"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Performance with vendor libraries on Trestles</td></tr></tbody></table></div>
<div class="p1"><br/>MKL still outperforms ACML even on AMD processors, but the margin is almost negligible.  As with the Gordon case though, the difference in MPI implementations is start because of OpenMPI's poor collective performance.<br/><br/>It is worth noting that PGI with OpenMPI did not work unless <i>both</i> of the following OpenMPI parameters were specified:</div>
<div class="p1"><br/></div>
<pre>OMPI_MCA_coll_sync_barrier_after=100<br/>OMPI_MCA_coll_sync_barrier_before=100</pre><div class="p1"><br/></div>
<div class="p1">At smaller processor counts, ScaLAPACK compiled with OpenMPI (both Netlib's and MKL's implementations) performed horrendously.  I don't know exactly what the conflict is, but OpenMPI and ScaLAPACK do not seem to play nicely.<br/><br/></div>
<h2>Netlib reference implementations</h2><div class="p1">As a fun afterthought, I thought it also might be useful to compare the vendor libraries to Netlib's reference implementations of BLAS and LAPACK.  I rebuilt the four compiler+MPI combinations on both systems using Netlib's BLAS, LAPACK, and ScaLAPACK (as well as the stock FFTW library instead of MKL or ACML's versions) to see how badly Netlib's reference really performs, and here are the results:</div>
<div class="p1"><br/><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-XrD1PtrJ_iI/UwASdyK5f_I/AAAAAAAAKR4/f13bCkKCte8/s1600/Gordon+Reference.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="271" src="http://3.bp.blogspot.com/-XrD1PtrJ_iI/UwASdyK5f_I/AAAAAAAAKR4/f13bCkKCte8/s1600/Gordon+Reference.png" width="400"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Performance with Netlib reference libraries on Gordon.  The build with Intel and MVAPICH2 was not able to run.</td></tr></tbody></table><br/>On SDSC's Gordon resource, the OpenMPI builds were between 3× and 4× slower, but the PGI build with MVAPICH2 was only(!) 64% slower.  This is a curious result, as I would have expected performance to be dramatically worse across all combinations of compiler and MPI library since BLAS and LAPACK should really show no performance difference when it comes to the choice of MPI library. <br/><br/>The above results suggest that Quantum ESPRESSO makes its heavy use of BLAS and LAPACK through the ScaLAPACK library, and as such, the ScaLAPACK implementation and its performance with each of the MPI libraries is critically important.  Of course, even with a good combination of ScaLAPACK and MPI stack, having a vendor-optimized BLAS and LAPACK goes a long way in increasing overall performance by more than 50%.<br/><br/>It should also be obvious that the Intel and MVAPICH2 build's performance data is absent.  This is because the build with Intel and MVAPICH2 repeatedly failed with this error:<br/><br/><pre>** On entry to DLASCL parameter number 4 had an illegal value</pre><br/>This error is the result of DGELSD within LAPACK not converging within the hard-coded criteria.  <a href="https://icl.cs.utk.edu/lapack-forum/viewtopic.php?t=529">This problem has been detailed at the LAPACK developers' forums</a>, and the limits were actually dramatically increased since the postings in the aforementioned forum. <br/><br/>Despite that patch though, the problem still manifests in the newest versions of Netlib's reference BLACS/ScaLAPACK implementation, and I suspect that this is really <a href="http://mailman.cse.ohio-state.edu/pipermail/mvapich-discuss/2013-May/004434.html">a fundamental limitation of the BLACS library relying on platform-dependent behavior to produce its results</a>.  Recall from above that the vendor-supplied implementations of LAPACK do not trigger this error.<br/><br/>On Trestles, the results are even worse:<br/><br/><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-Gu-zuZeCanw/UwASr7WVTAI/AAAAAAAAKR8/FxMmVFKNWoo/s1600/Trestles+Reference.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="271" src="http://4.bp.blogspot.com/-Gu-zuZeCanw/UwASr7WVTAI/AAAAAAAAKR8/FxMmVFKNWoo/s1600/Trestles+Reference.png" width="400"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Performance with Netlib reference libraries on Trestles.  Only the build with PGI and MVAPICH2 was able to run.</td></tr></tbody></table>When built with the Intel compiler, both MVAPICH2- and OpenMPI-linked builds trigger the DLASCL error.  The PGI and OpenMPI build do not trigger this error, but instead hang on collectives even with the OpenMPI tunings I reported for the vendor-optimized Trestles PGI+OpenMPI build.<br/><br/>Cranking up the implicit barrier frequency beyond 100 might have gotten the test to run, but quite frankly, having to put a barrier before <i>and</i> after every 100th collective is already an extremely aggressive modification to runtime behavior.  Ultimately, this data all suggests that you should, in fact, never use the Netlib reference implementations of BLAS and LAPACK.</div>
<div class="p1"><br/></div>
<h2>Summary of Data</h2><div class="p1">Here is an overall summary of the test matrix:</div>
<div class="p1"><br/><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-a9IQN7AJdeU/UwATBXIp5bI/AAAAAAAAKSE/FFJ8mvqUc3s/s1600/Comparison+All.png" style="margin-left: auto; margin-right: auto;"><img border="0" height="271" src="http://1.bp.blogspot.com/-a9IQN7AJdeU/UwATBXIp5bI/AAAAAAAAKSE/FFJ8mvqUc3s/s1600/Comparison+All.png" width="400"/></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Overall performance comparison for AUSURF112 benchmark</td></tr></tbody></table><br/>This benchmark is very sensitive to the performance of collectives, and exactly how collectives are performed is specific to the MPI implementation being used.  OpenMPI shows weaker collective performance across the board, and as a result, significantly worse performance.<br/><br/>These collective calls are largely made via the ScaLAPACK library though, and since ScaLAPACK is built upon BLAS and LAPACK, it is critical to have all components (BLAS, LAPACK, ScaLAPACK, and the MPI implementation) working together.  In all cases tested, <b>Intel's MKL library along with MVAPICH2 provides the best performance</b>.  As one may guess, ACML also performs well on AMD Opteron processors, but its lack of optimization for AVX instructions prevented it from realizing the full performance possible on Sandy Bridge processors.<br/><br/>In addition to performance, there are conclusions to be drawn about <i>application resiliency</i>, or Quantum ESPRESSO's ability to run calculations without hanging or throwing strange errors:<br/><ul><li><b>PGI with MVAPICH2 was the most resilient combination</b>; it worked out of the box with all combinations of BLAS/LAPACK/ScaLAPACK tested</li><li><b>PGI with OpenMPI was the least resilient combination</b>, perhaps because ACML's lack of ScaLAPACK bindings forces the use of Netlib ScaLAPACK.  Combining Netlib BLAS/LAPACK/ScaLAPACK with PGI/OpenMPI simply failed on Trestles, and getting Netlib ScaLAPACK to play nicely with either MKL or ACML's BLAS/LAPACK libraries when compiled against PGI and OpenMPI required tuning of the OpenMPI collectives.</li><li>In both test systems, <b>using vendor libraries wherever possible make Quantum ESPRESSO run more reliably</b>.  The only roadblocks encountered when using MKL or ACML arose when they were combined with PGI and OpenMPI, where special collective tunings had to be done.</li></ul><br/>At the end of the day, there aren't many big surprises here.  There are three take-away lessons:<br/><ol><li>MKL provides very strong optimizations for the Intel x86 architecture, and ACML isn't so bad either.  You run into trouble when you start linking against Netlib libraries.</li><li>MVAPICH2 has better collectives than OpenMPI, and this translates into better ScaLAPACK performance.  Again, this becomes less true when you start linking against Netlib libraries.</li><li><b>Don't use the Netlib reference implementations of BLAS, LAPACK, or ScaLAPACK</b> because they aren't designed for performance or resiliency.  </li><ul><li><b>Using Netlib caused performance to drop by between 60% and 400%</b>, and </li><li><b>only half of the builds that linked against the Netlib reference trials would even run</b>.</li></ul></ol><div>Friends don't let friends link against Netlib!</div>
</div>
>